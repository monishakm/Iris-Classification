{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.2","name":"python","mimetype":"text/x-python","nbconvert_exporter":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# K Nearest Neighbor ","metadata":{}},{"cell_type":"markdown","source":"### 1. KNN Theory","metadata":{}},{"cell_type":"markdown","source":"#### 1.1 Type of algorithm","metadata":{}},{"cell_type":"markdown","source":"KNN can be used for both classification and regression predictive problems. KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations $(x,y)$ and would like to capture the relationship between $x$ and $y$. More formally, our goal is to learn a function $h: X\\rightarrow Y$ so that given an unseen observation $x$, $h(x)$ can confidently predict the corresponding output $y$.\n","metadata":{}},{"cell_type":"markdown","source":"#### 1.2 Distance measure","metadata":{}},{"cell_type":"markdown","source":"In the classification setting, the K-nearest neighbor algorithm essentially boils down to forming a majority vote between the K most similar instances to a given “unseen” observation. Similarity is defined according to a distance metric between two data points. The k-nearest-neighbor classifier is commonly based on the Euclidean distance between a test sample and the specified training samples. Let $x_{i}$ be an input sample with $p$ features $(x_{i1}, x_{i2},..., x_{ip})$, $n$ be the total number of input samples $(i=1,2,...,n)$.  The Euclidean distance between sample $x_{i}$ and $x_{l}$ is is defined as: \n\n\n$$d(x_{i}, x_{l}) = \\sqrt{(x_{i1} - x_{l1})^2 + (x_{i2} - x_{l2})^2 + ... + (x_{ip} - x_{lp})^2}$$\n\nSometimes other measures can be more suitable for a given setting and include the Manhattan, Chebyshev and Hamming distance.","metadata":{}},{"cell_type":"markdown","source":"#### 1.3 Algorithm steps","metadata":{}},{"cell_type":"markdown","source":"STEP 1: Cgoose the number K of neighbors\n\nSTEP 2: Take the K nearest neighbors of the new data point, according to your distance metric\n\nSTEP 3: Among these K neighbors, count the number of data points to each category\n\nSTEP 4: Assign the new data point to the category where you counted the most neighbors","metadata":{}},{"cell_type":"markdown","source":"### 2. Importing and preperation of data","metadata":{}},{"cell_type":"markdown","source":"#### 2.1 Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Load dataset","metadata":{}},{"cell_type":"markdown","source":"NOTE: Iris dataset includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.","metadata":{}},{"cell_type":"code","source":"# Importing the dataset\ndataset = pd.read_csv('../input/Iris.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3 Summarize the Dataset","metadata":{}},{"cell_type":"code","source":"# We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\ndataset.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let’s now take a look at the number of instances (rows) that belong to each class. We can view this as an absolute count.\ndataset.groupby('Species').size()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4 Dividing data into features and labels","metadata":{}},{"cell_type":"markdown","source":"NOTE: As we can see dataset contain six columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm and Species. The actual features are described by columns 1-4. Last column contains labels of samples. Firstly we need to split data into two arrays: X (features) and y (labels).","metadata":{}},{"cell_type":"code","source":"feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm','PetalWidthCm']\nX = dataset[feature_columns].values\ny = dataset['Species'].values\n\n# Alternative way of selecting features and labels arrays:\n# X = dataset.iloc[:, 1:5].values\n# y = dataset.iloc[:, 5].values","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.5 Label encoding","metadata":{}},{"cell_type":"markdown","source":"NOTE: As we can see labels are categorical. KNeighborsClassifier does not accept string labels. We need to use LabelEncoder to transform them into numbers. Iris-setosa correspond to 0, Iris-versicolor correspond to 1 and Iris-virginica correspond to 2.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.6 Spliting dataset into training set and test set","metadata":{}},{"cell_type":"markdown","source":"Let's split dataset into training set and test set, to check later on whether or not our classifier works correctly.","metadata":{}},{"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, because features values are in the same order of magnitude, there is no need for feature scaling. Nevertheless in other sercostamses it is extremly important to apply feature scaling before running classification algorythms.","metadata":{}},{"cell_type":"markdown","source":"### 3. Data Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1. Parallel Coordinates","metadata":{}},{"cell_type":"markdown","source":"Parallel coordinates is a plotting technique for plotting multivariate data. It allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together.","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import parallel_coordinates\nplt.figure(figsize=(15,10))\nparallel_coordinates(dataset.drop(\"Id\", axis=1), \"Species\")\nplt.title('Parallel Coordinates Plot', fontsize=20, fontweight='bold')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Features values', fontsize=15)\nplt.legend(loc=1, prop={'size': 15}, frameon=True,shadow=True, facecolor=\"white\", edgecolor=\"black\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2. Andrews Curves","metadata":{}},{"cell_type":"markdown","source":"Andrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures.","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import andrews_curves\nplt.figure(figsize=(15,10))\nandrews_curves(dataset.drop(\"Id\", axis=1), \"Species\")\nplt.title('Andrews Curves Plot', fontsize=20, fontweight='bold')\nplt.legend(loc=1, prop={'size': 15}, frameon=True,shadow=True, facecolor=\"white\", edgecolor=\"black\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3. Pairplot","metadata":{}},{"cell_type":"markdown","source":"Pairwise is useful when you want to visualize the distribution of a variable or the relationship between multiple variables separately within subsets of your dataset.","metadata":{}},{"cell_type":"code","source":"plt.figure()\nsns.pairplot(dataset.drop(\"Id\", axis=1), hue = \"Species\", size=3, markers=[\"o\", \"s\", \"D\"])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.4. Boxplots","metadata":{}},{"cell_type":"code","source":"plt.figure()\ndataset.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(15, 10))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.5. 3D visualization","metadata":{}},{"cell_type":"markdown","source":"You can also try to  visualize high-dimensional datasets in 3D using color, shape, size and other properties of 3D and 2D objects. In this plot I used marks sizes to visualize fourth dimenssion which is Petal Width [cm].","metadata":{}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(1, figsize=(20, 15))\nax = Axes3D(fig, elev=48, azim=134)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y,\n           cmap=plt.cm.Set1, edgecolor='k', s = X[:, 3]*50)\n\nfor name, label in [('Virginica', 0), ('Setosa', 1), ('Versicolour', 2)]:\n    ax.text3D(X[y == label, 0].mean(),\n              X[y == label, 1].mean(),\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'),size=25)\n\nax.set_title(\"3D visualization\", fontsize=40)\nax.set_xlabel(\"Sepal Length [cm]\", fontsize=25)\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"Sepal Width [cm]\", fontsize=25)\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Petal Length [cm]\", fontsize=25)\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Using KNN for classification","metadata":{}},{"cell_type":"markdown","source":"#### 4.1. Making predictions","metadata":{}},{"cell_type":"code","source":"# Fitting clasifier to the Training set\n# Loading libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate learning model (k = 3)\nclassifier = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2. Evaluating predictions","metadata":{}},{"cell_type":"markdown","source":"Building confusion matrix:","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\ncm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating model accuracy:","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3. Using cross-validation for parameter tuning:","metadata":{}},{"cell_type":"code","source":"# creating list of K for KNN\nk_list = list(range(1,50,2))\n# creating list of cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\nplt.xlabel('Number of Neighbors K', fontsize=15)\nplt.ylabel('Misclassification Error', fontsize=15)\nsns.set_style(\"whitegrid\")\nplt.plot(k_list, MSE)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding best k\nbest_k = k_list[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d.\" % best_k)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. My own KNN implementation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy as sp\n\nclass MyKNeighborsClassifier():\n    \"\"\"\n    My implementation of KNN algorithm.\n    \"\"\"\n    \n    def __init__(self, n_neighbors=5):\n        self.n_neighbors=n_neighbors\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as array of features and y as array of labels.\n        \"\"\"\n        n_samples = X.shape[0]\n        # number of neighbors can't be larger then number of samples\n        if self.n_neighbors > n_samples:\n            raise ValueError(\"Number of neighbors can't be larger then number of samples in training set.\")\n        \n        # X and y need to have the same number of samples\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"Number of samples in X and y need to be equal.\")\n        \n        # finding and saving all possible class labels\n        self.classes_ = np.unique(y)\n        \n        self.X = X\n        self.y = y\n        \n    def predict(self, X_test):\n        \n        # number of predictions to make and number of features inside single sample\n        n_predictions, n_features = X_test.shape\n        \n        # allocationg space for array of predictions\n        predictions = np.empty(n_predictions, dtype=int)\n        \n        # loop over all observations\n        for i in range(n_predictions):\n            # calculation of single prediction\n            predictions[i] = single_prediction(self.X, self.y, X_test[i, :], self.n_neighbors)\n\n        return(predictions)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def single_prediction(X, y, x_train, k):\n    \n    # number of samples inside training set\n    n_samples = X.shape[0]\n    \n    # create array for distances and targets\n    distances = np.empty(n_samples, dtype=np.float64)\n\n    # distance calculation\n    for i in range(n_samples):\n        distances[i] = (x_train - X[i]).dot(x_train - X[i])\n    \n    # combining arrays as columns\n    distances = sp.c_[distances, y]\n    # sorting array by value of first column\n    sorted_distances = distances[distances[:,0].argsort()]\n    # celecting labels associeted with k smallest distances\n    targets = sorted_distances[0:k,1]\n\n    unique, counts = np.unique(targets, return_counts=True)\n    return(unique[np.argmax(counts)])","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate learning model (k = 3)\nmy_classifier = MyKNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model\nmy_classifier.fit(X_train, y_train)\n\n# Predicting the Test set results\nmy_y_pred = my_classifier.predict(X_test)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, my_y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Bibliography\n\n1. MIT Lecture: https://www.youtube.com/watch?v=09mb78oiPkA\n2. Iris dataset: https://www.kaggle.com/uciml/iris\n3. Theory: http://www.scholarpedia.org/article/K-nearest_neighbor\n\n4. https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/\n5. https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\n6. https://www.analyticsvidhya.com/blog/2014/10/introduction-k-neighbours-algorithm-clustering/","metadata":{}}]}